{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7d86f0f0-e320-479f-84ee-1fe0bae6c62f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#!pip install pyEDFlib\n",
    "import pyedflib\n",
    "#!pip install ipympl\n",
    "\n",
    "from scipy.fftpack import fft, ifft, fftfreq\n",
    "from scipy import signal\n",
    "from scipy.ndimage.filters import gaussian_filter1d, gaussian_filter\n",
    "from scipy.stats import binned_statistic, entropy, norm\n",
    "from statsmodels.stats.multitest import multipletests\n",
    "import statsmodels.api as sm\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "import sys\n",
    "import os\n",
    "import time\n",
    "#import pickle\n",
    "import dill as pickle\n",
    "\n",
    "import concurrent.futures\n",
    "from numba import jit, njit, prange\n",
    "\n",
    "from tqdm.notebook import tqdm\n",
    "from collections import defaultdict\n",
    "import itertools\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "from utility_functions import *\n",
    "from class_lfp import LFP\n",
    "from class_pac import MyPAC\n",
    "\n",
    "class Patient:\n",
    "    \n",
    "    def __init__(self, name, root_dir, channels=None, sampling_frequency=2000):\n",
    "        \n",
    "        print(\"List of things to make sure before analysis: \")\n",
    "        print(\"1) .bdf files are in patient folder (root_dir)\")\n",
    "        print(\"2) annotation files share the same name as .bdf files but with _annotations.txt suffix\")\n",
    "        print(\"3) annotations share the same naming principle: e.g. 1Day OFF RH (Com)\")\n",
    "        \n",
    "        self.name = name\n",
    "        self.root_dir = root_dir\n",
    "        \n",
    "        self.files = set()\n",
    "        self.file_conditions = defaultdict(dict)\n",
    "        self.file_annotations = {} # key = file, value = dataframe\n",
    "        self.conditions = set()\n",
    "        self.condition_durations = {}\n",
    "        self.placements = set()\n",
    "        self.comments = set()\n",
    "        \n",
    "        self.lfp = defaultdict(dict) # self.lfp[condition][placement]\n",
    "        self.pac = defaultdict(lambda: defaultdict(lambda: defaultdict(dict))) # pac[condition][placement_phase][placement_ampl]\n",
    "        \n",
    "        self.sf=sampling_frequency\n",
    "        \n",
    "        if channels is None:\n",
    "            ch_names = ['R1', 'R2C', 'R2B', 'R2A', 'R3C', 'R3B', 'R3A', 'R4', \\\n",
    "                        'L1', 'L2C', 'L2B', 'L2A', 'L3C', 'L3B', 'L3A', 'L4']\n",
    "            ch_indexes = range(9, 25)\n",
    "            self.channels = {ch_idx: ch_name for (ch_idx, ch_name) in zip(ch_indexes, ch_names)}\n",
    "        else:\n",
    "            self.channels = channels\n",
    "        \n",
    "        self.ch_names = list(self.channels.values())\n",
    "        self.ch_indexes = list(self.channels.keys())\n",
    "        \n",
    "    \n",
    "\n",
    "    def get_preprocessed_lfps(self, formula='abc-extended', verbose=True):\n",
    "        \"\"\"\n",
    "        MUST update self.file_conditions for each file BEFORE using this method!\n",
    "        ### Use p.add_file('filename')\n",
    "        ### p.file_condition('filename', condition_name, condition_duration)\n",
    "\n",
    "        *** WHAT THIS FUNCTION DOES ***\n",
    "        > For each file:\n",
    "        1) get_signals(file) [default channels: 1, 2abc, 3abc, 4]\n",
    "        2) get_bipolar_signals (using a predefined formulas)\n",
    "        3) updates placements using formula (e.g. 2A-3A)\n",
    "        For each placement and condition (of that file):\n",
    "            a) adds LFP[condition][placement] instance to Patient instance\n",
    "            b) removes 50Hz harmonics\n",
    "            c) filters 4-999 Hz using bandpass\n",
    "        > Finally it merges file_conditions into ONE self.conditions_durations\n",
    "\n",
    "        We get self.lfps updated: all bipolar LFPs for each condition and placement preprocessed\n",
    "        AND we get all placements and conditions: self.placements, self.conditions, self.conditions_durations \n",
    "        \"\"\"\n",
    "\n",
    "        for file in self.files:\n",
    "            # for each file we have: conditions, condition_durations\n",
    "            placements, bipolar_signals = self.get_bipolar_signals(*self.get_signals(os.path.join(self.root_dir,file)), formula=formula)\n",
    "            self.placements = set(placements)\n",
    "            for placement in tqdm(self.placements, desc='Creating LFPs'):\n",
    "                data = bipolar_signals[placement] # SHOULD BE A DICT\n",
    "                for condition in self.file_conditions[file].keys():\n",
    "                    # reading file_conditions for timestamps\n",
    "                    begin, end = self.file_conditions[file][condition]\n",
    "                    sf = self.sf\n",
    "                    begin *= sf\n",
    "                    end *= sf\n",
    "                    if verbose: print(\"Adding {} {} LFP, {}-{} sec\".format(condition, placement, begin/sf, end/sf))\n",
    "                    self.lfp[condition][placement] = LFP(data[int(begin):int(end)], sf, patient_name=self.name, condition=condition, placement=placement)\n",
    "                    # preprocessing lfps\n",
    "                    lfp = self.lfp[condition][placement]\n",
    "                    lfp.remove_50hz_harmonics(70, inplace=True)\n",
    "                    lfp.bp_filter(4, 999, inplace=True, filter_order=3)\n",
    "\n",
    "            del(bipolar_signals)\n",
    "    # making conditions normal again (union over \"file\" axis) \n",
    "        for file in self.files:\n",
    "            self.condition_durations.update(self.file_conditions[file])\n",
    "        self.conditions = set(self.condition_durations.keys())\n",
    "        \n",
    "         \n",
    "    def get_signals(self, file, ch_nrs=None):\n",
    "        t0 = time.time()\n",
    "        filepath = os.path.join(self.root_dir, file)\n",
    "        print(f\"Started reading {filepath}\")\n",
    "        if ch_nrs is None:\n",
    "            ch_nrs = self.ch_indexes\n",
    "        print(f\"Channels: {ch_nrs}\")    \n",
    "        signals, signal_headers, header = pyedflib.highlevel.read_edf(filepath, ch_nrs=self.ch_indexes, verbose=True)\n",
    "        for i, signal_header in zip(self.ch_indexes, signal_headers):\n",
    "            print(signal_header['label'])\n",
    "            \n",
    "        # IMPLEMENT CHANNEL RENAMING?\n",
    "        sf = signal_headers[0]['sample_rate']\n",
    "        print(\"Sampling frequency: \", sf)\n",
    "        if sf != self.sf:\n",
    "            q = int(sf/self.sf)\n",
    "            print(\"Downsampling by the factor \", q)\n",
    "            new_signals, new_signal_headers = downsample(signals, signal_headers, 8)\n",
    "            print(\"New sampling frequency: \", sf/q)   \n",
    "        else:\n",
    "            new_signals, new_signal_headers = signals, signal_headers      \n",
    "        t1 = time.time()   \n",
    "        print(\"Reading done, {} sec\".format(round(t1 - t0, 1)))    \n",
    "        return new_signals, new_signal_headers   \n",
    "    \n",
    "    \n",
    "    def get_bipolar_signals(self, signals, signal_headers, formula='abc'):\n",
    "        \"\"\"\n",
    "        Returns bipolar_signals:dict, bipolar_signals_names:list\n",
    "        \n",
    "        \"\"\"\n",
    "       # ch_names = ['R1', 'R2C', 'R2B', 'R2A', 'R3C', 'R3B', 'R3A', 'R4', \\\n",
    "       #             'L1', 'L2C', 'L2B', 'L2A', 'L3C', 'L3B', 'L3A', 'L4']\n",
    "        if formula == 'abc':\n",
    "            substraction_pairs = [(1, 4), (2, 5), (3, 6), (9, 12), (10, 13), (11, 14)]\n",
    "            # 2a-3a, 2b-3b, 2c-3c\n",
    "        elif formula == 'abc-extended':\n",
    "            substraction_pairs = [(0, 1), (0, 2), (0, 3), \\\n",
    "                                  (1, 2), (2, 3), (3, 1), \\\n",
    "                                  (1, 4), (2, 5), (3, 6), \\\n",
    "                                  (4, 5), (5, 6), (6, 4), \\\n",
    "                                  (7, 4), (7, 5), (7, 6), \\\n",
    "                                  (8, 9), (8, 10), (8, 11), \\\n",
    "                                  (9, 10), (10, 11), (11, 9), \\\n",
    "                                  (9, 12), (10, 13), (11, 14), \\\n",
    "                                  (12, 13), (13, 14), (14, 12), \\\n",
    "                                  (15, 12), (15, 13), (15, 14)]\n",
    "        # 1 - 2a, 1 - 2b, 1 - 3c, abc-pairs, all 2x pairs, 3x pairs, 4-3a, 4-3b, 4-3c\n",
    "        print(\"Started creating bipolar signals\")\n",
    "        print(\"\")\n",
    "        # CALCULATION STEP\n",
    "        bip_sig_names = []\n",
    "        bipolar_signals = dict()\n",
    "        for idx_1, idx_2 in substraction_pairs:\n",
    "            bip_sig_name = f\"{self.ch_names[idx_1]}-{self.ch_names[idx_2][1:]}\" # e.g. R2A-3A\n",
    "            bip_sig_names.append(bip_sig_name)\n",
    "            bipolar_signals[bip_sig_name] = signals[idx_1] - signals[idx_2]\n",
    "            print(f\"{bip_sig_name} created\")\n",
    "            \n",
    "        return bip_sig_names, bipolar_signals\n",
    "    \n",
    "    \n",
    "    def add_file(self, filename):\n",
    "        self.files.add(filename)\n",
    "        \n",
    "        \n",
    "    def find_bdf_files(self):\n",
    "        folder = self.root_dir\n",
    "        print(f\"Looking for .bdf files in {folder}\")\n",
    "        bdf_files = [f for f in os.listdir(folder) if f[-3:] == 'bdf']\n",
    "        print(f\"Found {bdf_files}\")\n",
    "        self.files.update(bdf_files)\n",
    "        return bdf_files\n",
    "        \n",
    "    \n",
    "    def file_condition(self, file, condition_label, timestamps):\n",
    "        self.file_conditions[file][condition_label] = timestamps\n",
    "        \n",
    "        \n",
    "    def scan_file_annotations(self, file, update_file_conditions=False):\n",
    "        annot_file = file[:-4] + \"_annotations.txt\"\n",
    "        print(\"Reading \", annot_file)\n",
    "        filepath = os.path.join(self.root_dir, annot_file)\n",
    "        df = pd.read_csv(filepath, header=0, sep=';')\n",
    "        # annots\n",
    "        mask = []\n",
    "        for i in range(len(df)):\n",
    "            flag = False\n",
    "            if type(df.loc[i, 'Annotation']) is str:\n",
    "                if 'Day' in df.loc[i, 'Annotation']:\n",
    "                    flag = True\n",
    "            mask.append(flag)\n",
    "            \n",
    "        df_annot = df[mask].reset_index(drop=True)\n",
    "        \n",
    "        # Creating columns: Day, L-DOPA, Condition (State, Movement)\n",
    "        df_annot['Day'] = df_annot['Annotation'].apply(lambda s: s.split(\" \")[0])\n",
    "        df_annot['L-DOPA'] = df_annot['Annotation'].apply(lambda s: s.split(\" \")[1])\n",
    "        df_annot['State'] = df_annot['Annotation'].apply(lambda s: s.split(\" \")[2:])\n",
    "        \n",
    "        print(df_annot)\n",
    "        \n",
    "        self.file_annotations[file] = df_annot\n",
    "        if update_file_conditions:\n",
    "            self.annotations_to_file_conditions(file, df_annot)\n",
    "        return df_annot\n",
    "    \n",
    "    \n",
    "    def annotations_to_file_conditions(self, file, df):\n",
    "        for i in range(len(df)):\n",
    "            onset = df.loc[i, 'Onset']\n",
    "            duration = df.loc[i, 'Duration']\n",
    "            condition = df.loc[i, 'Annotation']\n",
    "            self.file_condition(file, condition, (onset, onset + duration))\n",
    "        print(\"Updated self.file_condition with annotations from file\")\n",
    "    \n",
    "                                \n",
    "    def condition(self, condition_label, timestamps):\n",
    "        \"\"\"\n",
    "        label: condition label e.g. \"1Day OFF Rest\"\n",
    "        timestamps: (begin, end) in seconds\n",
    "        \"\"\"\n",
    "        self.condition_durations[condition_label] = timestamps\n",
    "        self.conditions.add(condition_label)\n",
    "    \n",
    "    \n",
    "    def merge_annotations(self):\n",
    "        self.annotations = pd.concat([self.file_annotations[file] for file in self.files], axis=0)\n",
    "        return self.annotations\n",
    "\n",
    "    \n",
    "    def merge_conditions(self, conditions_to_merge:list, new_condition_name, total_duration=180):\n",
    "        \"\"\"\n",
    "        Gets list of condition names as input and for each placement:\n",
    "        1) Merges them creating and adding the corresponding LFP with new_condition_name for Patient\n",
    "        2) Makes sure the total duration of LFP is equals total_duration\n",
    "\n",
    "        * Make sure LFPs for conditions_to_merge are computed!\n",
    "        * Make sure there is data of necessary duration (sum(durations) >= total_duration)\n",
    "        * If len(conditions_to_merge) == 1: skips the first step\n",
    "        \"\"\"\n",
    "        self.conditions.add(new_condition_name)\n",
    "        for placement in self.placements:\n",
    "            lfps_to_merge = [self.lfp[condition][placement] for condition in conditions_to_merge]\n",
    "            data_to_merge = [lfp.data for lfp in lfps_to_merge]\n",
    "            new_data = np.concatenate(data_to_merge)[:int(total_duration*self.sf)]\n",
    "            lfp = LFP(new_data, self.sf, self.name, new_condition_name, placement)\n",
    "            \n",
    "            # too many prints\n",
    "            verbose = False\n",
    "            if placement == 'R2A-3A':\n",
    "                verbose = True\n",
    "            self.add_lfp(lfp, verbose=verbose)\n",
    "\n",
    "            \n",
    "        \n",
    "    def display_all_annotations(self):\n",
    "        df = self.merge_annotations()\n",
    "        days = [\"1Day\", \"5Day\"]\n",
    "        ldopas = [\"OFF\", \"ON\"]\n",
    "        for day in days:\n",
    "            print(f\"----{day}----\")\n",
    "            for ldopa in ldopas:\n",
    "                print(f\"------{ldopa}------\")\n",
    "                mask = (df[\"Day\"] == day) & (df[\"L-DOPA\"] == ldopa)\n",
    "                print(df[mask].loc[:, [\"Onset\", \"Duration\", \"State\"]])\n",
    "                \n",
    "        \n",
    "    \n",
    "    def add_lfp(self, lfp: LFP, verbose=True):\n",
    "        \"\"\"\n",
    "        Adds lfp to patient instance lfp[condition][placement]\n",
    "        Use correct condition and placement in lfp attributes!\n",
    "        \"\"\"\n",
    "        condition, placement = lfp.condition, lfp.placement\n",
    "        if verbose: print(f\"Adding LFP to {self.name} object. \\nCondition: {condition} \\nPlacement: {placement}\")\n",
    "        self.lfp[condition][placement] = lfp\n",
    "        if verbose: print(\"Updating condition\")\n",
    "        self.conditions.add(condition)\n",
    "        \n",
    "        \n",
    "    def add_lfp_deprecated(self, data, sf, condition, placement):\n",
    "        \"\"\"\n",
    "        Writes lfp instance into a double indexed dictionary\n",
    "        from (bipolar) data according to condition (begin, end) duration\n",
    "        self.lfp[condition][placement]\n",
    "        \n",
    "        \"\"\"\n",
    "        raise Exception(\"Deprecated. The only way to do this is to use self.get_preprocessed_lfps()\")\n",
    "        begin, end = self.condition_durations[condition]\n",
    "        \n",
    "        begin *= sf\n",
    "        end *= sf\n",
    "        print(\"Adding {} {} LFP, {}-{} sec\".format(condition, placement, begin/sf, end/sf))\n",
    "        lfp = LFP(data[begin:end], sf, patient=self.name, condition=condition, placement=placement)\n",
    "        self.lfp[condition][placement] = lfp\n",
    "        \n",
    "        \n",
    "    def add_pac(self, pac_obj):\n",
    "        \"\"\"\n",
    "        Adds pac object to triple-nested self.pac dictionary using pac_ojb.lfp_phase and lfp_amplitude\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        lfp_phase, lfp_amplitude = pac_obj.lfp_phase, pac_obj.lfp_amplitude\n",
    "        \n",
    "        assert (self.name == lfp_phase.patient_name)\n",
    "        condition = lfp_phase.condition\n",
    "        placement_phase = lfp_phase.placement\n",
    "        placement_amplitude = lfp_amplitude.placement\n",
    "        \n",
    "        self.pac[condition][placement_phase][placement_amplitude] = pac_obj\n",
    "        \n",
    "        \n",
    "    def load_pac(self, filepath=None, condition=None, phase_placement=None, ampl_placement=None, duration=None):\n",
    "        pac_root_dir = os.path.join(self.root_dir, 'pac')\n",
    "        if filepath is None:\n",
    "            assert condition is not None, \"If filepath is not specified other parameters should be given\"\n",
    "            name_components = [self.name, \n",
    "                               condition, \n",
    "                               phase_placement, \n",
    "                               ampl_placement, \n",
    "                               f\"{duration} sec\"]\n",
    "            \n",
    "            filepath = os.path.join(pac_root_dir, '_'.join(name_components) + '.pkl')   \n",
    "        print(f\"Reading {filepath}\") \n",
    "        with open(filepath, 'rb') as _input:\n",
    "            pac = pickle.load(_input)\n",
    "        print(\"MyPAC object loaded.\")\n",
    "            \n",
    "        self.pac[condition][phase_placement][ampl_placement] = pac\n",
    "        \n",
    "        print(f\"Updated {self.name} pac.[condition][phase_placement][amplitude_placement]\")\n",
    "        print(\"Returning loaded pac object\")\n",
    "            \n",
    "        return pac\n",
    "    \n",
    "    \n",
    "    def load_all_pacs(self):\n",
    "        for filename in os.listdir(os.path.join(self.root_dir, 'pac')):\n",
    "            self.load_pac(os.path.join(self.root_dir, 'pac', filename))\n",
    "            \n",
    "              \n",
    "    def comment(self, comment):\n",
    "        self.comments.add(comment)\n",
    "        \n",
    "        \n",
    "    def info(self):\n",
    "        attributes = [a for a in dir(self) if not a.startswith('__')]\n",
    "        for attribute in attributes:\n",
    "            print(f\"{attribute}: {self.__dict__[attribute]}\")\n",
    "            \n",
    "    \n",
    "    def __getstate__(self):\n",
    "        attrs = self.__dict__.copy()\n",
    "        keys_to_del = ['pac']\n",
    "        print(f\"Pickling {self.name} without {keys_to_del}\")\n",
    "        for key in keys_to_del:\n",
    "            del attrs[key]\n",
    "        return attrs\n",
    "    \n",
    "    \n",
    "    #def __setstate__(self, d):\n",
    "    #    self.__dict__ = d\n",
    "    #    self.__dict__['pac'] = defaultdict(lambda: defaultdict(lambda: defaultdict(dict)))\n",
    "        \n",
    "            \n",
    "            \n",
    "    def save(self, filename=None):\n",
    "        t = time.time()\n",
    "        if filename is None:\n",
    "            filename = self.name + \".pkl\"\n",
    "        filepath = os.path.join(self.root_dir, filename)\n",
    "        self.pickle_filepath = filepath\n",
    "        print(f\"Saving {self.name} object to {filepath} ...\")\n",
    "        with open(filepath, 'wb') as output:\n",
    "            pickle.dump(self, output, pickle.HIGHEST_PROTOCOL)\n",
    "        print(f\"Done, {time.time() - t} sec\")\n",
    "        file_stats = os.stat(filepath)\n",
    "        print(f'File size: {file_stats.st_size / (1024 * 1024)} MB')\n",
    "        print(\"Returning filepath for saved file\")\n",
    "        return filepath\n",
    "    \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b0b5d61b-ce58-4393-8c2f-2a02cf3884e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "List of things to make sure before analysis: \n",
      "1) .bdf files are in patient folder (root_dir)\n",
      "2) annotation files share the same name as .bdf files but with _annotations.txt suffix\n",
      "3) annotations share the same naming principle: e.g. 1Day OFF RH (Com)\n"
     ]
    }
   ],
   "source": [
    "p = Patient(\"PTEST\", r\"C:\\Users\\aleks\\[[Python]]\\LAB\\data\\Patient2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a282f1b3-61e5-4c78-be51-75b3d1b197b2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading C:\\Users\\aleks\\[[Python]]\\LAB\\data\\Patient2\\pac\\PAC_Patient2_1Day OFF Rest 180sec_L1-2A_L1-2A_180.0 sec.pkl\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pac_class'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_25848/3871729068.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_all_pacs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_25848/976008048.py\u001b[0m in \u001b[0;36mload_all_pacs\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    359\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mload_all_pacs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    360\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mfilename\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mroot_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'pac'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 361\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_pac\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mroot_dir\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'pac'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    362\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    363\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_25848/976008048.py\u001b[0m in \u001b[0;36mload_pac\u001b[1;34m(self, filepath, condition, phase_placement, ampl_placement, duration)\u001b[0m\n\u001b[0;32m    346\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34mf\"Reading {filepath}\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    347\u001b[0m         \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfilepath\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'rb'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0m_input\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 348\u001b[1;33m             \u001b[0mpac\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_input\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    349\u001b[0m         \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"MyPAC object loaded.\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    350\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\pac\\lib\\site-packages\\dill\\_dill.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(file, ignore, **kwds)\u001b[0m\n\u001b[0;32m    268\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    269\u001b[0m     \u001b[1;34m\"\"\"unpickle an object from a file\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 270\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mUnpickler\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mignore\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    271\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    272\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mloads\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mignore\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\pac\\lib\\site-packages\\dill\\_dill.py\u001b[0m in \u001b[0;36mload\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    471\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    472\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;31m#NOTE: if settings change, need to update attributes\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 473\u001b[1;33m         \u001b[0mobj\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mStockUnpickler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    474\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__module__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_main_module\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'__name__'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'__main__'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    475\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_ignore\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\envs\\pac\\lib\\site-packages\\dill\\_dill.py\u001b[0m in \u001b[0;36mfind_class\u001b[1;34m(self, module, name)\u001b[0m\n\u001b[0;32m    461\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mtype\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#XXX: special case: NoneType missing\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    462\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m'dill.dill'\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mmodule\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'dill._dill'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 463\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mStockUnpickler\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfind_class\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodule\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    464\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    465\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m__init__\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pac_class'"
     ]
    }
   ],
   "source": [
    "p.load_all_pacs()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3152236-5d68-4462-afc4-47d16592afb7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
